{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raaghashree/Pattern-Based-Insider-Threat-Detection-Using-Machine-Learning/blob/main/XGBoost_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGh82lX1Prwx",
        "outputId": "c1e24c05-e33f-46b7-c74a-691d49143d39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "fQEKYye8mBMY",
        "outputId": "3b6cc613-8d26-4ed3-9339-9b951b7f4c7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-57a3e1099f37>:27: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.dropna(thresh=df.shape[0] * 0.5, axis=1, inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Non-numeric columns found: ['log_type', 'access_label_warning_access', 'is_weekend_True', 'is_night_True', 'is_off_hours_True', 'action_create', 'action_delete', 'action_deployment_trigger', 'action_exec', 'action_failed_login', 'action_file_access', 'action_file_modify', 'action_login', 'action_logout', 'action_scale', 'action_sudo_command']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "Non-numeric data found in columns: ['log_type', 'access_label_warning_access', 'is_weekend_True', 'is_night_True', 'is_off_hours_True', 'action_create', 'action_delete', 'action_deployment_trigger', 'action_exec', 'action_failed_login', 'action_file_access', 'action_file_modify', 'action_login', 'action_logout', 'action_scale', 'action_sudo_command']",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-57a3e1099f37>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;31m# Ensure all features are numeric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnon_numeric_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Non-numeric data found in columns: {non_numeric_cols}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;31m# Step 11: Separate features and target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Non-numeric data found in columns: ['log_type', 'access_label_warning_access', 'is_weekend_True', 'is_night_True', 'is_off_hours_True', 'action_create', 'action_delete', 'action_deployment_trigger', 'action_exec', 'action_failed_login', 'action_file_access', 'action_file_modify', 'action_login', 'action_logout', 'action_scale', 'action_sudo_command']"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ipaddress\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Load dataset\n",
        "try:\n",
        "    df = pd.read_csv(\"unified_logs_dataset.csv\")\n",
        "    print(f\"Dataset loaded successfully with {df.shape[0]} rows and {df.shape[1]} columns\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'Malwaredata1.csv' file not found. Please check the file path.\")\n",
        "    exit(1)\n",
        "# Step 1: Clean Column Names\n",
        "df.columns = df.columns.str.strip()\n",
        "\n",
        "# Step 2: Handle Missing Values\n",
        "missing_percentage = df.isnull().mean() * 100\n",
        "print(f\"Columns with missing values: {missing_percentage[missing_percentage > 0].to_dict()}\")\n",
        "\n",
        "# Drop columns with >50% missing values\n",
        "df.dropna(thresh=len(df) * 0.5, axis=1, inplace=True)\n",
        "\n",
        "# Fill numeric columns with mean, non-numeric with mode\n",
        "numeric_cols = df.select_dtypes(include=['number']).columns\n",
        "df[numeric_cols] = df[numeric_cols].apply(lambda col: col.fillna(col.mean()) if col.isnull().sum() > 0 else col)\n",
        "\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "df[categorical_cols] = df[categorical_cols].apply(lambda col: col.fillna(col.mode()[0]) if col.isnull().sum() > 0 else col)\n",
        "\n",
        "# Step 3: Remove Duplicate Rows\n",
        "duplicates = df.duplicated().sum()\n",
        "print(f\"Found {duplicates} duplicate rows\")\n",
        "df.drop_duplicates(inplace=True)\n",
        "\n",
        "# Step 4: Handle Non-Numeric Columns\n",
        "# Identify non-numeric columns\n",
        "non_numeric_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "print(\"Original non-numeric columns:\", non_numeric_cols)\n",
        "\n",
        "# Specify columns to keep among non-numeric: 'Label', 'Source_IP', 'Destination_IP', 'Protocol_Type', 'System_Patch_Status'\n",
        "keep_columns = ['Label', 'Source_IP', 'Destination_IP', 'Protocol_Type', 'System_Patch_Status']\n",
        "# Drop all non-numeric columns that are not in keep_columns\n",
        "cols_to_drop = [col for col in non_numeric_cols if col not in keep_columns]\n",
        "print(f\"Non-numeric columns that will be dropped: {cols_to_drop}\")\n",
        "df.drop(columns=cols_to_drop, inplace=True, errors='ignore')\n",
        "\n",
        "# Convert IP addresses into numeric form\n",
        "if 'Source_IP' in df.columns:\n",
        "    df['Source_IP_int'] = df['Source_IP'].apply(lambda ip: int(ipaddress.IPv4Address(ip)))\n",
        "if 'Destination_IP' in df.columns:\n",
        "    df['Destination_IP_int'] = df['Destination_IP'].apply(lambda ip: int(ipaddress.IPv4Address(ip)))\n",
        "# Drop original IP columns\n",
        "df.drop(columns=['Source_IP', 'Destination_IP'], inplace=True)\n",
        "\n",
        "# Encode 'Protocol_Type' using LabelEncoder\n",
        "if 'Protocol_Type' in df.columns:\n",
        "    proto_encoder = LabelEncoder()\n",
        "    df['Protocol_Type'] = proto_encoder.fit_transform(df['Protocol_Type'])\n",
        "    print(\"Encoded 'Protocol_Type'.\")\n",
        "\n",
        "# âœ… Encode 'System_Patch_Status' using LabelEncoder\n",
        "if 'System_Patch_Status' in df.columns:\n",
        "    print(f\"Encoding 'System_Patch_Status'. Original values: {df['System_Patch_Status'].unique()}\")\n",
        "    patch_encoder = LabelEncoder()\n",
        "    df['System_Patch_Status'] = patch_encoder.fit_transform(df['System_Patch_Status'])\n",
        "    print(f\"Encoded values: {df['System_Patch_Status'].unique()}\")\n",
        "\n",
        "# Step 5: Encode Target Variable\n",
        "if 'Label' in df.columns:\n",
        "    if df['Label'].dtype == 'object':\n",
        "        print(f\"Encoding target variable. Original values: {df['Label'].unique()}\")\n",
        "        encoder = LabelEncoder()\n",
        "        df['Label'] = encoder.fit_transform(df['Label'])\n",
        "        print(f\"Encoded values: {df['Label'].unique()}\")\n",
        "else:\n",
        "    print(\"Error: Target column 'Label' not found in dataset\")\n",
        "    exit(1)\n",
        "\n",
        "# Step 6: Ensure All Features Are Numeric\n",
        "X = df.drop(columns=['Label'])\n",
        "y = df['Label']\n",
        "\n",
        "# ðŸ’¡ Function to clean inf values in a specific column\n",
        "def clean_feature_column(X):\n",
        "    col = 'Normalized_Packet_Flow'\n",
        "    if col in X.columns:\n",
        "        mean_val = X[col][~np.isinf(X[col])].mean()\n",
        "        X[col] = X[col].replace([np.inf, -np.inf], mean_val)\n",
        "    return X\n",
        "\n",
        "# âœ… Apply the fix for 'inf' values\n",
        "X = clean_feature_column(X)\n",
        "\n",
        "# Step 7: Handle Class Imbalance using SMOTE\n",
        "class_counts = y.value_counts()\n",
        "print(f\"Class distribution before SMOTE: {class_counts.to_dict()}\")\n",
        "imbalance_ratio = class_counts.min() / class_counts.max()\n",
        "print(f\"Class imbalance ratio: {imbalance_ratio:.2f}\")\n",
        "\n",
        "if imbalance_ratio < 0.5:\n",
        "    print(\"Applying SMOTE to balance classes...\")\n",
        "    smote = SMOTE(sampling_strategy=1.0, random_state=42)  # Fully balance classes\n",
        "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "    print(f\"Data shape after SMOTE: {X_resampled.shape}\")\n",
        "else:\n",
        "    print(\"Class balance is acceptable, skipping SMOTE\")\n",
        "    X_resampled, y_resampled = X, y\n",
        "\n",
        "# Convert back to DataFrame\n",
        "df_balanced = pd.DataFrame(X_resampled, columns=X.columns)\n",
        "df_balanced['Label'] = y_resampled\n",
        "\n",
        "# Check new class distribution\n",
        "print(f\"Class distribution after SMOTE: {df_balanced['Label'].value_counts().to_dict()}\")\n",
        "\n",
        "# Step 8: Normalize Features using MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "X_normalized = scaler.fit_transform(df_balanced.drop(columns=['Label']))\n",
        "df_processed = pd.DataFrame(X_normalized, columns=X.columns)\n",
        "df_processed['Label'] = df_balanced['Label']\n",
        "\n",
        "# Save the cleaned and preprocessed dataset\n",
        "df_processed.to_csv(\"Processed_CTDAPD_Dataset.csv\", index=False)\n",
        "print(\"\\nData Preprocessing Complete! Cleaned dataset saved as 'Processed_CTDAPD_Dataset.csv'.\")\n",
        "print(f\"Final dataset shape: {df_processed.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "new"
      ],
      "metadata": {
        "id": "iJKzAwhdqPXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import ipaddress\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Helper functions\n",
        "def safe_ip_to_int(ip):\n",
        "    try:\n",
        "        return int(ipaddress.IPv4Address(ip))\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "def hash_encode(x, modulo=10000):\n",
        "    return hash(x) % modulo\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"unified_logs_dataset.csv\")\n",
        "\n",
        "# Step 1: Clean column names\n",
        "df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')\n",
        "\n",
        "# Step 2: Drop duplicate rows\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "# Step 3: Drop columns with >50% missing values\n",
        "df.dropna(thresh=df.shape[0] * 0.5, axis=1, inplace=True)\n",
        "\n",
        "# Step 4: Handle missing values\n",
        "if 'suspicious' in df.columns:\n",
        "    df = df.dropna(subset=['suspicious'])\n",
        "\n",
        "numeric_cols = df.select_dtypes(include=['number']).columns\n",
        "df[numeric_cols] = df[numeric_cols].apply(lambda col: col.fillna(col.mean()))\n",
        "\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "df[categorical_cols] = df[categorical_cols].apply(\n",
        "    lambda col: col.fillna(col.mode().iloc[0]) if not col.mode().empty else col.fillna(\"missing\")\n",
        ")\n",
        "\n",
        "# Step 5: Encode IP address\n",
        "if 'ip' in df.columns:\n",
        "    df['ip_int'] = df['ip'].apply(safe_ip_to_int)\n",
        "    df.drop(columns=['ip'], inplace=True)\n",
        "\n",
        "# Step 6: Timestamp Feature Engineering\n",
        "if 'timestamp' in df.columns:\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "\n",
        "    df['hour'] = df['timestamp'].dt.hour\n",
        "    df['dayofweek'] = df['timestamp'].dt.dayofweek\n",
        "    df['is_weekend'] = df['dayofweek'] >= 5\n",
        "    df['is_night'] = df['hour'].between(0, 6)\n",
        "    df['is_off_hours'] = ~df['hour'].between(8, 18)\n",
        "\n",
        "    def label_access(row):\n",
        "        if row['is_weekend'] or row['is_night'] or row['is_off_hours']:\n",
        "            return 'warning_access'\n",
        "        return 'normal_access'\n",
        "\n",
        "    df['access_label'] = df.apply(label_access, axis=1)\n",
        "    df.drop(columns=['timestamp'], inplace=True)\n",
        "\n",
        "# Step 7: Encode categorical features\n",
        "high_card_cols = ['user', 'resource', 'container']\n",
        "low_card_cols = ['action', 'protocol', 'pipeline']\n",
        "\n",
        "# Label Encoding for low-cardinality categorical features\n",
        "le = LabelEncoder()\n",
        "\n",
        "for col in low_card_cols:\n",
        "    if col in df.columns:\n",
        "        df[col] = le.fit_transform(df[col].astype(str))\n",
        "\n",
        "# Hash Encoding for high-cardinality categorical features\n",
        "for col in high_card_cols:\n",
        "    if col in df.columns:\n",
        "        df[col + '_hash'] = df[col].apply(lambda x: hash_encode(x))\n",
        "        df.drop(columns=[col], inplace=True)\n",
        "\n",
        "# Step 8: Encode 'log_type' and 'access_label' as necessary\n",
        "for col in ['log_type', 'access_label']:\n",
        "    if col in df.columns:\n",
        "        df[col] = le.fit_transform(df[col].astype(str))\n",
        "\n",
        "# Step 9: Encode 'suspicious' if needed\n",
        "if 'suspicious' in df.columns and df['suspicious'].dtype != 'int64':\n",
        "    df['suspicious'] = pd.factorize(df['suspicious'])[0]\n",
        "\n",
        "# Step 10: Drop irrelevant columns\n",
        "df.drop(columns=[col for col in ['session_id'] if col in df.columns], inplace=True)\n",
        "\n",
        "# Step 11: Ensure all features are numeric\n",
        "# Ensure that all columns are numeric\n",
        "df = df.apply(pd.to_numeric, errors='ignore')\n",
        "\n",
        "# Check for non-numeric columns\n",
        "non_numeric_cols = [col for col in df.columns if not np.issubdtype(df[col].dtype, np.number)]\n",
        "if non_numeric_cols:\n",
        "    print(f\"Non-numeric columns found: {non_numeric_cols}\")\n",
        "else:\n",
        "    print(\"All features are numeric.\")\n",
        "\n",
        "# Step 12: Separate features and target\n",
        "X = df.drop(\"suspicious\", axis=1)\n",
        "y = df[\"suspicious\"].astype(int)\n",
        "\n",
        "# Step 13: Handle class imbalance using SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "# Step 14: Normalize features using MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X_resampled)\n",
        "\n",
        "# Step 15: Save cleaned dataset\n",
        "cleaned_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
        "cleaned_df['suspicious'] = y_resampled\n",
        "cleaned_df.to_csv(\"preprocessed_dataset.csv\", index=False)\n",
        "\n",
        "print(\"âœ… Full preprocessing complete. Output saved to 'preprocessed_dataset.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5E2afV1nqNzz",
        "outputId": "6b5415a9-4784-49c9-886e-46efba9d3b6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-4af330466896>:96: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
            "  df = df.apply(pd.to_numeric, errors='ignore')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Non-numeric columns found: ['is_weekend', 'is_night', 'is_off_hours']\n",
            "âœ… Full preprocessing complete. Output saved to 'preprocessed_dataset.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "df = pd.read_csv(\"unified_logs_dataset_preprocessed.csv\")  # Replace with the actual file path\n",
        "\n",
        "# Step 2: Rename or assign to df_processed if needed\n",
        "df_processed = df.copy()\n",
        "\n",
        "# Step 3: Separate features and label\n",
        "X = df_processed.drop(columns=['suspicious'])  # 'suspicious' is your label column\n",
        "y = df_processed['suspicious']\n",
        "\n",
        "# Step 4: Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training set shape: {X_train.shape}, Training labels: {y_train.shape}\")\n",
        "print(f\"Testing set shape: {X_test.shape}, Testing labels: {y_test.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYgNWnSn1aZi",
        "outputId": "c9ab836f-07b1-4140-a698-d7fed78a569e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set shape: (23995, 22), Training labels: (23995,)\n",
            "Testing set shape: (5999, 22), Testing labels: (5999,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import joblib\n",
        "\n",
        "# Step 1: Feature & Label separation\n",
        "X = df_processed.drop(columns=['suspicious'])  # <-- correct column name\n",
        "y = df_processed['suspicious']\n",
        "\n",
        "# Step 2: Train-Test Split (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Step 3: Initialize and Train XGBoost Classifier\n",
        "xgb_model = xgb.XGBClassifier(\n",
        "    objective='binary:logistic',\n",
        "    eval_metric='logloss',\n",
        "    use_label_encoder=False,\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=6,\n",
        "    random_state=42\n",
        ")\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict\n",
        "y_pred = xgb_model.predict(X_test)\n",
        "\n",
        "# Step 5: Evaluation\n",
        "print(\"\\nâœ… XGBoost Model Evaluation:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Step 6: Save model\n",
        "joblib.dump(xgb_model, \"xgboost_unified_logs_dataset_preprocessed.pkl\")\n",
        "print(\"\\nðŸš€ Trained XGBoost model saved as 'unified_logs_dataset_preprocessed.pkl'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "ve9JVqSK3rxI",
        "outputId": "e7a0e149-b007-4c4c-ee93-0ca60e3498bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:session_id: object, log_type: object",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-a1bff32cf7a1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m )\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mxgb_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Step 4: Predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 726\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[1;32m   1578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1579\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_configure_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1580\u001b[0;31m             train_dmatrix, evals = _wrap_evaluation_matrices(\n\u001b[0m\u001b[1;32m   1581\u001b[0m                 \u001b[0mmissing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1582\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36m_wrap_evaluation_matrices\u001b[0;34m(missing, X, y, group, qid, sample_weight, base_margin, feature_weights, eval_set, sample_weight_eval_set, base_margin_eval_set, eval_group, eval_qid, create_dmatrix, enable_categorical, feature_types)\u001b[0m\n\u001b[1;32m    601\u001b[0m     \"\"\"Convert array_like evaluation matrices into DMatrix.  Perform validation on the\n\u001b[1;32m    602\u001b[0m     way.\"\"\"\n\u001b[0;32m--> 603\u001b[0;31m     train_dmatrix = create_dmatrix(\n\u001b[0m\u001b[1;32m    604\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m         \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36m_create_dmatrix\u001b[0;34m(self, ref, **kwargs)\u001b[0m\n\u001b[1;32m   1063\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_can_use_qdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_method\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbooster\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"gblinear\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1065\u001b[0;31m                 return QuantileDMatrix(\n\u001b[0m\u001b[1;32m   1066\u001b[0m                     \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnthread\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_bin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_bin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 726\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, label, weight, base_margin, missing, silent, feature_names, feature_types, nthread, max_bin, ref, group, qid, label_lower_bound, label_upper_bound, feature_weights, enable_categorical, data_split_mode)\u001b[0m\n\u001b[1;32m   1571\u001b[0m                 )\n\u001b[1;32m   1572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1573\u001b[0;31m         self._init(\n\u001b[0m\u001b[1;32m   1574\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1575\u001b[0m             \u001b[0mref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_init\u001b[0;34m(self, data, ref, enable_categorical, **meta)\u001b[0m\n\u001b[1;32m   1630\u001b[0m             \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1631\u001b[0m         )\n\u001b[0;32m-> 1632\u001b[0;31m         \u001b[0mit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1633\u001b[0m         \u001b[0;31m# delay check_call to throw intermediate exception first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1634\u001b[0m         \u001b[0m_check_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    567\u001b[0m             \u001b[0mexc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m  \u001b[0;31m# pylint: disable=raising-bad-type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__del__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[0;34m(self, fn, dft_ret)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0;31m# Defer the exception in order to return 0 and stop the iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_temporary_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m         \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/data.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m   1400\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mit\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1402\u001b[0;31m         \u001b[0minput_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1403\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 726\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minput_data\u001b[0;34m(data, feature_names, feature_types, **kwargs)\u001b[0m\n\u001b[1;32m    615\u001b[0m                 \u001b[0mnew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat_codes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_temporary_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 617\u001b[0;31m                 new, cat_codes, feature_names, feature_types = _proxy_transform(\n\u001b[0m\u001b[1;32m    618\u001b[0m                     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m                     \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/data.py\u001b[0m in \u001b[0;36m_proxy_transform\u001b[0;34m(data, feature_names, feature_types, enable_categorical)\u001b[0m\n\u001b[1;32m   1445\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_arrow_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1446\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_pandas_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1447\u001b[0;31m         df, feature_names, feature_types = _transform_pandas_df(\n\u001b[0m\u001b[1;32m   1448\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menable_categorical\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1449\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/data.py\u001b[0m in \u001b[0;36m_transform_pandas_df\u001b[0;34m(data, enable_categorical, feature_names, feature_types, meta)\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0mmeta\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m ) -> Tuple[PandasTransformed, Optional[FeatureNames], Optional[FeatureTypes]]:\n\u001b[0;32m--> 603\u001b[0;31m     \u001b[0mpandas_check_dtypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menable_categorical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmeta\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmeta\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_matrix_meta\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"DataFrame for {meta} cannot have multiple columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/data.py\u001b[0m in \u001b[0;36mpandas_check_dtypes\u001b[0;34m(data, enable_categorical)\u001b[0m\n\u001b[1;32m    567\u001b[0m             \u001b[0;32mor\u001b[0m \u001b[0mis_pa_ext_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m         ):\n\u001b[0;32m--> 569\u001b[0;31m             \u001b[0m_invalid_dataframe_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_pd_sparse_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/data.py\u001b[0m in \u001b[0;36m_invalid_dataframe_dtype\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"DataFrame.dtypes for data must be int, float, bool or category.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\"\"{type_err} {_ENABLE_CAT_ERR} {err}\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:session_id: object, log_type: object"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "...\n",
        "# Step 6: Save model\n",
        "joblib.dump(xgb_model, \"xgboost_cyber_model.pkl\")\n",
        "print(\"\\nðŸš€ Trained XGBoost model saved as 'xgboost_cyber_model.pkl'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9hkisi6PSmv",
        "outputId": "6a6ecaca-7f9b-4131-c278-a7b7588e7d41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸš€ Trained XGBoost model saved as 'xgboost_cyber_model.pkl'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"xgboost_cyber_model.pkl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "nAoCiDwVRCVV",
        "outputId": "96e02386-b703-4d33-9973-027de78c7aea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_cc1cb4db-cdc8-4888-b4cf-7b19516fd279\", \"xgboost_cyber_model.pkl\", 456991)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NEW\n"
      ],
      "metadata": {
        "id": "Hj314MWc01s6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "def prepare_for_xgboost(df):\n",
        "    df = df.copy()\n",
        "\n",
        "    # Step 1: Drop columns not useful for model training\n",
        "    df.drop(columns=['username', 'source_ip'], inplace=True)\n",
        "\n",
        "    # Step 2: Extract datetime features from 'timestamp'\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
        "    df['hour'] = df['timestamp'].dt.hour\n",
        "    df['day_of_week'] = df['timestamp'].dt.dayofweek\n",
        "    df.drop(columns=['timestamp'], inplace=True)\n",
        "\n",
        "    # Step 3: Fill NA and encode categorical features\n",
        "    cat_cols = ['user_id', 'k8s_action', 'auth_event', 'cicd_action', 'time_of_day', 'ip_category']\n",
        "    le_dict = {}\n",
        "\n",
        "    for col in cat_cols:\n",
        "        df[col] = df[col].fillna('none')\n",
        "        le = LabelEncoder()\n",
        "        df[col] = le.fit_transform(df[col])\n",
        "        le_dict[col] = le  # Save encoders for future decoding if needed\n",
        "\n",
        "    # Step 4: Ensure all columns are numeric\n",
        "    df = df.apply(pd.to_numeric)\n",
        "\n",
        "    return df, le_dict\n"
      ],
      "metadata": {
        "id": "d0U_-O_C2jCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/My Drive/Dataset/FOR MODELS/insider_threat_detection_dataset.csv')\n",
        "processed_df, encoders = prepare_for_xgboost(df)\n",
        "\n",
        "# Split for XGBoost training\n",
        "X = processed_df.drop(columns=['label'])\n",
        "y = processed_df['label']\n"
      ],
      "metadata": {
        "id": "2xkiRrbN2ioB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# ------------------------\n",
        "# Step 1: Load the dataset\n",
        "# ------------------------\n",
        "df = pd.read_csv('/content/drive/My Drive/Dataset/FOR MODELS/insider_threat_detection_dataset.csv')\n",
        "\n",
        "# ------------------------\n",
        "# Step 2: Preprocess it\n",
        "'''\n",
        "# ------------------------\n",
        "def prepare_for_xgboost(df):\n",
        "\n",
        "    df = df.copy()\n",
        "\n",
        "    # Drop irrelevant columns\n",
        "    df.drop(columns=['username', 'source_ip'], inplace=True)\n",
        "\n",
        "    # Extract datetime features\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
        "    df['hour'] = df['timestamp'].dt.hour\n",
        "    df['day_of_week'] = df['timestamp'].dt.dayofweek\n",
        "    df.drop(columns=['timestamp'], inplace=True)\n",
        "\n",
        "    # Encode categorical columns\n",
        "    cat_cols = ['user_id', 'k8s_action', 'auth_event', 'cicd_action', 'time_of_day', 'ip_category']\n",
        "    for col in cat_cols:\n",
        "        df[col] = df[col].fillna('none')\n",
        "        le = LabelEncoder()\n",
        "        df[col] = le.fit_transform(df[col])\n",
        "\n",
        "    # Make sure all are numeric\n",
        "    df = df.apply(pd.to_numeric)\n",
        "\n",
        "    return df\n",
        "'''\n",
        "import numpy as np\n",
        "from scipy.stats import zscore\n",
        "\n",
        "def prepare_for_xgboost(df):\n",
        "    df = df.copy()\n",
        "\n",
        "    # Drop irrelevant\n",
        "    df.drop(columns=['username', 'source_ip'], inplace=True)\n",
        "\n",
        "    # Handle missing timestamps and convert\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
        "    df['hour'] = df['timestamp'].dt.hour.fillna(-1)\n",
        "    df['day_of_week'] = df['timestamp'].dt.dayofweek.fillna(-1)\n",
        "    df.drop(columns=['timestamp'], inplace=True)\n",
        "\n",
        "    # Encode user_id by frequency\n",
        "    df['user_id_freq'] = df['user_id'].map(df['user_id'].value_counts())\n",
        "    df.drop(columns=['user_id'], inplace=True)\n",
        "\n",
        "    # One-hot encode low-cardinality categorical features\n",
        "    low_card_cat_cols = ['k8s_action', 'auth_event', 'cicd_action', 'time_of_day', 'ip_category']\n",
        "    for col in low_card_cat_cols:\n",
        "        df[col] = df[col].fillna('none')\n",
        "    df = pd.get_dummies(df, columns=low_card_cat_cols, dummy_na=True)\n",
        "\n",
        "    # Temporal features\n",
        "    df['is_night'] = df['hour'].apply(lambda x: 1 if x < 6 or x > 22 else 0)\n",
        "    df['is_weekend'] = df['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)\n",
        "    df['hour_day_interaction'] = df['hour'] * df['day_of_week']\n",
        "\n",
        "    # Group-based statistics (e.g., number of records per user frequency)\n",
        "    df['records_per_user_freq'] = df['user_id_freq'].map(df['user_id_freq'].value_counts())\n",
        "\n",
        "    # Normalize user activity\n",
        "    df['user_activity_ratio'] = df['user_id_freq'] / df['user_id_freq'].sum()\n",
        "\n",
        "    # Z-score anomaly proxy (for any numerical feature with wide range)\n",
        "    for col in df.select_dtypes(include=[np.number]).columns:\n",
        "        if df[col].nunique() > 10:  # Avoid one-hot and binary columns\n",
        "            df[f'{col}_zscore'] = zscore(df[col].fillna(0))\n",
        "\n",
        "    # Optional: log-transform skewed columns\n",
        "    skewed_cols = ['user_id_freq', 'records_per_user_freq']\n",
        "    for col in skewed_cols:\n",
        "        df[f'{col}_log'] = np.log1p(df[col])\n",
        "\n",
        "    # Ensure all numeric\n",
        "    df = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
        "\n",
        "    return df\n",
        "\n",
        "df_processed = prepare_for_xgboost(df)\n",
        "\n",
        "# ------------------------\n",
        "# Step 3: Prepare features and label\n",
        "# ------------------------\n",
        "X = df_processed.drop(columns=['label'])\n",
        "y = df_processed['label']\n",
        "\n",
        "# ------------------------\n",
        "# Step 4: Train-test split\n",
        "# ------------------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# ------------------------\n",
        "# Step 5: Train XGBoost\n",
        "# ------------------------\n",
        "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# ------------------------\n",
        "# Step 6: Evaluate\n",
        "# ------------------------\n",
        "y_pred = xgb_model.predict(X_test)\n",
        "\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"\\nAccuracy Score:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEV3Bvrv3iRd",
        "outputId": "e7df3ffe-446f-4252-f0bd-9903364ffde5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            " [[154 140]\n",
            " [149 157]]\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.51      0.52      0.52       294\n",
            "           1       0.53      0.51      0.52       306\n",
            "\n",
            "    accuracy                           0.52       600\n",
            "   macro avg       0.52      0.52      0.52       600\n",
            "weighted avg       0.52      0.52      0.52       600\n",
            "\n",
            "\n",
            "Accuracy Score: 0.5183333333333333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [07:23:45] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "joblib\n"
      ],
      "metadata": {
        "id": "JmhQvcwyoSoa"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3NX9yjw_oUL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "...\n",
        "# Step 6: Save model\n",
        "joblib.dump(xgb_model, \"xgboost_model.pkl\")\n",
        "print(\"\\nðŸš€ Trained XGBoost model saved as 'xgboost_model.pkl'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fc5d0f9-0f46-4548-b97d-06349fa69d24",
        "id": "dNfUVkMDoYD9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸš€ Trained XGBoost model saved as 'xgboost_model.pkl'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"xgboost_model.pkl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "9975dd77-a118-4d4c-ed48-177f47aefc90",
        "id": "pxRlSshAoYD_"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_6cfb3667-81db-453b-94f0-0b884c695544\", \"xgboost_model.pkl\", 335549)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}